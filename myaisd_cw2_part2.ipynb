{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "AISD Coursework - Part B (Proxy training for Indian context) - PyTorch Version\n",
        "\n",
        "File: myaisd_cw2_part2.py\n",
        "\n",
        "This script implements a lightweight Attention U-Net in PyTorch and trains it on\n",
        "the LandCover.ai v1 dataset (5 classes: other, building, woodland, water, road).\n",
        "\n",
        "It covers:\n",
        "\n",
        "- Part A, Task 3:\n",
        "  * Identify a contextually relevant multi-class land-cover dataset.\n",
        "  * Provide a full preprocessing pipeline (large orthophotos → 512x512 tiles).\n",
        "\n",
        "- Part A, Task 4:\n",
        "  * Adapt the model architecture to a multi-class Attention U-Net (AttUNet).\n",
        "  * Implement a custom mIoU metric for multi-class segmentation.\n",
        "\n",
        "- Part A, Task 5:\n",
        "  * Train the adapted model for 12 epochs.\n",
        "  * Track training/validation loss and mean IoU.\n",
        "  * Save the best model checkpoint based on validation mIoU.\n",
        "\n",
        "This script is the fully-completed proxy training run for the Indian\n",
        "context when GPU limits prevented running the 25-epoch TensorFlow version\n",
        "in AISD_CW2_partB.py end-to-end."
      ],
      "metadata": {
        "id": "rteY6HR8j49r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dog1cyp2b4_y",
        "outputId": "6491f5fc-d77f-4c2d-ef44-e55fe4404624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFLx1kZL8EJt",
        "outputId": "5e659190-07dd-40c7-e2ba-dc161d06fc67",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rasterio==1.3.10 in /usr/local/lib/python3.12/dist-packages (1.3.10)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (25.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (2025.11.12)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (8.3.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (2.0.2)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (1.4.7)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (1.1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from rasterio==1.3.10) (75.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.12/dist-packages (from snuggs>=1.4.1->rasterio==1.3.10) (3.2.5)\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# SETUP: Mount Drive, install deps, set device & seeds\n",
        "\n",
        "!pip install rasterio==1.3.10\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set a base working directory (change if you like)\n",
        "BASE_DIR = \"/content\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data_landcoverai\")\n",
        "TILES_DIR = os.path.join(DATA_DIR, \"tiles_512\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(TILES_DIR, exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1C4M9NLmKMluSleqrs9jwELyhl725LLAN -O landcover_ai.zip\n",
        "!unzip landcover_ai.zip -d landcover_ai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6h9qxMwDzLP",
        "outputId": "517f59c5-7acb-419f-be53-b5d747e6e1cc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1C4M9NLmKMluSleqrs9jwELyhl725LLAN\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "unzip:  cannot find or open landcover_ai.zip, landcover_ai.zip.zip or landcover_ai.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/data_landcoverai/raw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzgUTR6TEZaq",
        "outputId": "6384a15b-e516-4073-f45d-19459ba3c01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/data_landcoverai/raw': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "\n",
        "zip_path = \"/content/data_landcoverai/raw/landcover_ai_v1.zip\"\n",
        "extract_dir = \"/content/data_landcoverai/raw/landcover_ai_v1\"\n",
        "\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(extract_dir)\n",
        "    print(\"Extracted successfully to:\", extract_dir)\n",
        "else:\n",
        "    print(\"Already extracted:\", extract_dir)\n",
        "\n",
        "!ls -R \"/content/data_landcoverai/raw/landcover_ai_v1\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "XBMILFADDDBD",
        "outputId": "3b03fd6e-9421-4475-d58b-c0944a8ca893",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data_landcoverai/raw/landcover_ai_v1.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3869150168.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracted successfully to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data_landcoverai/raw/landcover_ai_v1.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqhKtj7QGjod",
        "outputId": "524a686c-8f16-469e-f20d-de7b6a509eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Colab Notebooks', 'PartB_AttUNet_LandCoverAI_best.pth', 'landcoverai_saved']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_dir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mbaRYj5pFbS-",
        "outputId": "63e8fa08-c1e5-4b3b-e9b8-a64673c4081c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data_landcoverai/raw/landcover_ai_v1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMGS_DIR = os.path.join(extract_dir, \"images\")\n",
        "MASKS_DIR = os.path.join(extract_dir, \"masks\")\n",
        "\n",
        "print(\"IMGS_DIR =\", IMGS_DIR)\n",
        "print(\"MASKS_DIR =\", MASKS_DIR)\n",
        "\n",
        "!ls -l \"$IMGS_DIR\"\n",
        "!ls -l \"$MASKS_DIR\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUqRdPrLFeJD",
        "outputId": "d050aac3-8a59-4917-eb8c-1db67d48a134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMGS_DIR = /content/data_landcoverai/raw/landcover_ai_v1/images\n",
            "MASKS_DIR = /content/data_landcoverai/raw/landcover_ai_v1/masks\n",
            "ls: cannot access '/content/data_landcoverai/raw/landcover_ai_v1/images': No such file or directory\n",
            "ls: cannot access '/content/data_landcoverai/raw/landcover_ai_v1/masks': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, numpy as np\n",
        "from PIL import Image\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Correct paths\n",
        "IMGS_DIR = os.path.join(extract_dir, \"images\")\n",
        "MASKS_DIR = os.path.join(extract_dir, \"masks\")\n",
        "\n",
        "print(\"IMGS_DIR =\", IMGS_DIR)\n",
        "print(\"MASKS_DIR =\", MASKS_DIR)\n",
        "\n",
        "tiles_img_dir = os.path.join(TILES_DIR, \"images\")\n",
        "tiles_msk_dir = os.path.join(TILES_DIR, \"masks\")\n",
        "\n",
        "os.makedirs(tiles_img_dir, exist_ok=True)\n",
        "os.makedirs(tiles_msk_dir, exist_ok=True)\n",
        "\n",
        "img_files = sorted(glob.glob(os.path.join(IMGS_DIR, \"*.tif\")))\n",
        "print(\"Found\", len(img_files), \"orthophotos.\")\n",
        "\n",
        "tile_size = 512\n",
        "tile_count = 0\n",
        "\n",
        "for img_path in tqdm(img_files, desc=\"Tiling orthophotos\"):\n",
        "    fname = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    mask_path = os.path.join(MASKS_DIR, fname + \".tif\")\n",
        "\n",
        "    if not os.path.exists(mask_path):\n",
        "        print(\"Missing mask for\", fname)\n",
        "        continue\n",
        "\n",
        "    with rasterio.open(img_path) as src_img, rasterio.open(mask_path) as src_msk:\n",
        "        H, W = src_img.height, src_img.width\n",
        "        nx = W // tile_size\n",
        "        ny = H // tile_size\n",
        "\n",
        "        for iy in range(ny):\n",
        "            for ix in range(nx):\n",
        "                window = Window(ix * tile_size, iy * tile_size, tile_size, tile_size)\n",
        "\n",
        "                img_tile = src_img.read(window=window)\n",
        "                msk_tile = src_msk.read(1, window=window)\n",
        "\n",
        "                img_tile = np.transpose(img_tile, (1,2,0))\n",
        "                img_tile = np.clip(img_tile, 0,255).astype(np.uint8)\n",
        "                msk_tile = msk_tile.astype(np.uint8)\n",
        "\n",
        "                tile_id = f\"{fname}_{iy}_{ix}\"\n",
        "                Image.fromarray(img_tile).save(os.path.join(tiles_img_dir, tile_id+\".png\"))\n",
        "                Image.fromarray(msk_tile).save(os.path.join(tiles_msk_dir, tile_id+\".png\"))\n",
        "\n",
        "                tile_count += 1\n",
        "\n",
        "print(\"Total tiles generated:\", tile_count)\n",
        "print(\"Tile images:\", len(glob.glob(os.path.join(tiles_img_dir,'*.png'))))\n",
        "print(\"Tile masks:\", len(glob.glob(os.path.join(tiles_msk_dir,'*.png'))))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N64-J-WGE8vb",
        "outputId": "5bd87d9e-35cb-4538-ea91-137fce430979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMGS_DIR = /content/data_landcoverai/raw/landcover_ai_v1/images\n",
            "MASKS_DIR = /content/data_landcoverai/raw/landcover_ai_v1/masks\n",
            "Found 0 orthophotos.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tiling orthophotos: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tiles generated: 0\n",
            "Tile images: 0\n",
            "Tile masks: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# B. PyTorch Dataset & DataLoaders\n",
        "# ============================================================\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tiles_img_dir = \"/content/data_landcoverai/tiles_512/images\"\n",
        "tiles_msk_dir = \"/content/data_landcoverai/tiles_512/masks\"\n",
        "\n",
        "# Collect aligned image-mask pairs\n",
        "all_imgs = sorted(glob.glob(os.path.join(tiles_img_dir, \"*.png\")))\n",
        "all_msks = sorted(glob.glob(os.path.join(tiles_msk_dir, \"*.png\")))\n",
        "\n",
        "print(\"Total PNG tiles:\", len(all_imgs), len(all_msks))\n",
        "\n",
        "# Subsample to 3600 (3000 train, 600 val)\n",
        "max_total = 3600\n",
        "if len(all_imgs) > max_total:\n",
        "    idx = np.random.choice(len(all_imgs), max_total, replace=False)\n",
        "    all_imgs = [all_imgs[i] for i in idx]\n",
        "    all_msks = [all_msks[i] for i in idx]\n",
        "    print(f\"Subsampled to {max_total} tiles.\")\n",
        "\n",
        "# Train/Val split (3000/600)\n",
        "train_imgs, val_imgs, train_msks, val_msks = train_test_split(\n",
        "    all_imgs, all_msks, test_size=600, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train samples:\", len(train_imgs))\n",
        "print(\"Val samples:\", len(val_imgs))\n",
        "\n",
        "# Image normalization\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def mask_to_tensor(mask_img):\n",
        "    mask_np = np.array(mask_img, dtype=np.int64)\n",
        "    return torch.from_numpy(mask_np)\n",
        "\n",
        "class LandCoverAIDataset(Dataset):\n",
        "    def __init__(self, images, masks):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
        "        msk = Image.open(self.masks[idx])\n",
        "\n",
        "        img = img_transform(img)\n",
        "        msk = mask_to_tensor(msk)\n",
        "\n",
        "        return img, msk\n",
        "\n",
        "train_ds = LandCoverAIDataset(train_imgs, train_msks)\n",
        "val_ds   = LandCoverAIDataset(val_imgs, val_msks)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Dataloaders ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "YIoZOhx9LYmG",
        "outputId": "084bb416-4f9f-4094-bfb8-2bc162bca615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total PNG tiles: 0 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "test_size=600 should be either positive and smaller than the number of samples 0 or a float in the (0, 1) range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2166360272.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Train/Val split (3000/600)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m train_imgs, val_imgs, train_msks, val_msks = train_test_split(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mall_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_msks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m     ):\n\u001b[0;32m-> 2426\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2427\u001b[0m             \u001b[0;34m\"test_size={0} should be either positive and smaller\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m             \u001b[0;34m\" than the number of samples {1} or a float in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: test_size=600 should be either positive and smaller than the number of samples 0 or a float in the (0, 1) range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Lightweight Attention U-Net (PyTorch)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super().__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        # g: gating (decoder), x: skip (encoder)\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "class UpBlockAttn(nn.Module):\n",
        "    def __init__(self, in_ch, skip_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
        "        self.attn = AttentionBlock(F_g=out_ch, F_l=skip_ch, F_int=out_ch // 2)\n",
        "        self.conv = DoubleConv(out_ch + skip_ch, out_ch)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        # Pad if needed (for odd dims)\n",
        "        diffY = skip.size()[2] - x.size()[2]\n",
        "        diffX = skip.size()[3] - x.size()[3]\n",
        "        x = F.pad(x, [diffX // 2, diffX - diffX // 2,\n",
        "                      diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        skip = self.attn(x, skip)\n",
        "        x = torch.cat([skip, x], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class AttUNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=5, base_ch=32):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(n_channels, base_ch)\n",
        "\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(base_ch, base_ch * 2)\n",
        "        )\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(base_ch * 2, base_ch * 4)\n",
        "        )\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(base_ch * 4, base_ch * 8)\n",
        "        )\n",
        "        self.down4 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(base_ch * 8, base_ch * 16)\n",
        "        )\n",
        "\n",
        "        self.up1 = UpBlockAttn(base_ch * 16, base_ch * 8, base_ch * 8)\n",
        "        self.up2 = UpBlockAttn(base_ch * 8,  base_ch * 4, base_ch * 4)\n",
        "        self.up3 = UpBlockAttn(base_ch * 4,  base_ch * 2, base_ch * 2)\n",
        "        self.up4 = UpBlockAttn(base_ch * 2,  base_ch,     base_ch)\n",
        "\n",
        "        self.outc = nn.Conv2d(base_ch, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)        # base\n",
        "        x2 = self.down1(x1)     # 2*base\n",
        "        x3 = self.down2(x2)     # 4*base\n",
        "        x4 = self.down3(x3)     # 8*base\n",
        "        x5 = self.down4(x4)     # 16*base\n",
        "\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x,  x3)\n",
        "        x = self.up3(x,  x2)\n",
        "        x = self.up4(x,  x1)\n",
        "\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "# LandCover.ai v1 has 5 classes: 0 other, 1 building, 2 woodland, 3 water, 4 road\n",
        "n_classes = 5\n",
        "\n",
        "# If i'll use CUDA OOM later, change base_ch=32, base_ch=16\n",
        "model = AttUNet(n_channels=3, n_classes=n_classes, base_ch=32).to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "print(f\"Model built. Total parameters: {total_params:.2f}M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSq2Lao4Lo4D",
        "outputId": "2d8135ae-211f-425f-abb3-6292751dc887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model built. Total parameters: 7.85M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "n_classes = 5\n",
        "best_model_path = \"/content/drive/MyDrive/PartB_AttUNet_LandCoverAI_best.pth\"\n",
        "\n",
        "# Rebuild EXACTLY the same architecture\n",
        "model = AttUNet(n_channels=3, n_classes=n_classes, base_ch=32).to(device)\n",
        "\n",
        "# Load weights\n",
        "state = torch.load(best_model_path, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded best model from:\", best_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "KiGL-XmkFOso",
        "outputId": "f0a97897-5f45-4d4b-9269-0764aa4d8600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AttUNet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3335186062.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Rebuild EXACTLY the same architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AttUNet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D. Training loop (12 epochs, save best model to Drive)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# FIXED: remove verbose argument\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=2\n",
        ")\n",
        "\n",
        "def compute_iou(pred, target, num_classes):\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = (pred == cls)\n",
        "        target_inds = (target == cls)\n",
        "        intersection = (pred_inds & target_inds).sum().item()\n",
        "        union = (pred_inds | target_inds).sum().item()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(intersection / union)\n",
        "    if len(ious) == 0:\n",
        "        return 0.0\n",
        "    return float(np.mean(ious))\n",
        "\n",
        "best_val_iou = 0.0\n",
        "best_model_path = \"/content/drive/MyDrive/PartB_AttUNet_LandCoverAI_best.pth\"\n",
        "\n",
        "epochs = 12\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # ---------- TRAIN ----------\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_iou = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\")\n",
        "    for imgs, masks in pbar:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        masks = masks.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        train_iou += compute_iou(preds.cpu(), masks.cpu(), n_classes)\n",
        "        num_batches += 1\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            \"loss\": train_loss / num_batches,\n",
        "            \"mIoU\": train_iou / num_batches\n",
        "        })\n",
        "\n",
        "    avg_train_loss = train_loss / num_batches\n",
        "    avg_train_iou = train_iou / num_batches\n",
        "\n",
        "    # ---------- VALIDATION ----------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_iou = 0.0\n",
        "    num_val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\"):\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            masks = masks.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_iou += compute_iou(preds.cpu(), masks.cpu(), n_classes)\n",
        "            num_val_batches += 1\n",
        "\n",
        "    avg_val_loss = val_loss / num_val_batches\n",
        "    avg_val_iou = val_iou / num_val_batches\n",
        "\n",
        "    scheduler.step(avg_val_iou)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{epochs} | \"\n",
        "        f\"TrainLoss={avg_train_loss:.4f}, Train mIoU={avg_train_iou:.4f} | \"\n",
        "        f\"ValLoss={avg_val_loss:.4f}, Val mIoU={avg_val_iou:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_iou > best_val_iou:\n",
        "        best_val_iou = avg_val_iou\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"  >> New best model saved with Val mIoU = {best_val_iou:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(\"Best Val mIoU:\", best_val_iou)\n",
        "print(\"Best model path:\", best_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5iTWLlEMOOC",
        "outputId": "46eff367-cb63-4306-e35f-4ac79c2a9774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.63it/s, loss=0.841, mIoU=0.331]\n",
            "Epoch 1/12 [Val]: 100%|██████████| 300/300 [00:23<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 | TrainLoss=0.8414, Train mIoU=0.3312 | ValLoss=0.7230, Val mIoU=0.2933\n",
            "  >> New best model saved with Val mIoU = 0.2933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.63it/s, loss=0.727, mIoU=0.355]\n",
            "Epoch 2/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/12 | TrainLoss=0.7269, Train mIoU=0.3549 | ValLoss=0.6178, Val mIoU=0.3940\n",
            "  >> New best model saved with Val mIoU = 0.3940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/12 [Train]: 100%|██████████| 1500/1500 [05:25<00:00,  4.62it/s, loss=0.642, mIoU=0.344]\n",
            "Epoch 3/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/12 | TrainLoss=0.6418, Train mIoU=0.3439 | ValLoss=0.7141, Val mIoU=0.2980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/12 [Train]: 100%|██████████| 1500/1500 [05:23<00:00,  4.63it/s, loss=0.586, mIoU=0.337]\n",
            "Epoch 4/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/12 | TrainLoss=0.5860, Train mIoU=0.3372 | ValLoss=0.7294, Val mIoU=0.2728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/12 [Train]: 100%|██████████| 1500/1500 [05:23<00:00,  4.64it/s, loss=0.557, mIoU=0.32]\n",
            "Epoch 5/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/12 | TrainLoss=0.5567, Train mIoU=0.3204 | ValLoss=0.6460, Val mIoU=0.3207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.62it/s, loss=0.502, mIoU=0.338]\n",
            "Epoch 6/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/12 | TrainLoss=0.5023, Train mIoU=0.3378 | ValLoss=0.5627, Val mIoU=0.3723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.62it/s, loss=0.482, mIoU=0.336]\n",
            "Epoch 7/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/12 | TrainLoss=0.4816, Train mIoU=0.3360 | ValLoss=0.6181, Val mIoU=0.3702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/12 [Train]: 100%|██████████| 1500/1500 [05:23<00:00,  4.63it/s, loss=0.465, mIoU=0.338]\n",
            "Epoch 8/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/12 | TrainLoss=0.4647, Train mIoU=0.3377 | ValLoss=0.6673, Val mIoU=0.2863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.62it/s, loss=0.445, mIoU=0.349]\n",
            "Epoch 9/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/12 | TrainLoss=0.4454, Train mIoU=0.3485 | ValLoss=0.6023, Val mIoU=0.3089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.63it/s, loss=0.437, mIoU=0.35]\n",
            "Epoch 10/12 [Val]: 100%|██████████| 300/300 [00:23<00:00, 12.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/12 | TrainLoss=0.4369, Train mIoU=0.3503 | ValLoss=0.5339, Val mIoU=0.3517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.63it/s, loss=0.427, mIoU=0.356]\n",
            "Epoch 11/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/12 | TrainLoss=0.4270, Train mIoU=0.3565 | ValLoss=0.6046, Val mIoU=0.2879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/12 [Train]: 100%|██████████| 1500/1500 [05:24<00:00,  4.62it/s, loss=0.409, mIoU=0.356]\n",
            "Epoch 12/12 [Val]: 100%|██████████| 300/300 [00:22<00:00, 13.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/12 | TrainLoss=0.4092, Train mIoU=0.3563 | ValLoss=0.6379, Val mIoU=0.2825\n",
            "Training finished.\n",
            "Best Val mIoU: 0.393999537232429\n",
            "Best model path: /content/drive/MyDrive/PartB_AttUNet_LandCoverAI_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sentinelhub import SHConfig\n",
        "\n",
        "#config = SHConfig()\n",
        "\n",
        "#config.sh_client_id = \"d81449d9-0e75-4df1-8d63-10778ce9e07d\"\n",
        "#config.sh_client_secret = \"d81449d9-0e75-4df1-8d63-10778ce9e07d\"\n",
        "\n",
        "#config.save()   # saves into ~/.config/sentinelhub/config.json\n",
        "\n",
        "#print(\"ID =\", config.sh_client_id)\n",
        "#print(\"Secret =\", config.sh_client_secret)\n",
        "\n",
        "#not using htis approahc as too many complexities arising with joshimath dataset fetch"
      ],
      "metadata": {
        "id": "plabMHOnnhPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchgeo\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchgeo.datasets import LandCoverAI\n",
        "\n",
        "root_dir = \"/content/landcoverai\"\n",
        "\n",
        "dataset = LandCoverAI(\n",
        "    root=root_dir,\n",
        "    split=\"train\",\n",
        "    download=True\n",
        ")\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZrwwkL8CHh78",
        "outputId": "577651d8-9ea7-452b-8994-5e141c4dcd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchgeo in /usr/local/lib/python3.12/dist-packages (0.8.0)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (0.8.1)\n",
            "Requirement already satisfied: geopandas>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (1.1.1)\n",
            "Requirement already satisfied: jsonargparse>=4.25 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]>=4.25->torchgeo) (4.44.0)\n",
            "Requirement already satisfied: kornia>=0.8.2 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (0.8.2)\n",
            "Requirement already satisfied: lightly!=1.4.26,>=1.4.5 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (1.5.22)\n",
            "Requirement already satisfied: lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (2.6.0)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.2 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (11.3.0)\n",
            "Requirement already satisfied: pyproj>=3.4 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (3.7.2)\n",
            "Requirement already satisfied: rasterio>=1.4.3 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (1.4.3)\n",
            "Requirement already satisfied: segmentation-models-pytorch>=0.5 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (0.5.0)\n",
            "Requirement already satisfied: shapely>=2 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (2.1.2)\n",
            "Requirement already satisfied: timm>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (1.0.22)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchmetrics>=1.2 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (1.8.2)\n",
            "Requirement already satisfied: torchvision>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (0.24.0+cu126)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from torchgeo) (4.15.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.12.1->torchgeo) (0.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.12.1->torchgeo) (25.0)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.12/dist-packages (from jsonargparse>=4.25->jsonargparse[signatures]>=4.25->torchgeo) (6.0.3)\n",
            "Requirement already satisfied: docstring-parser>=0.17 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]>=4.25->torchgeo) (0.17.0)\n",
            "Requirement already satisfied: typeshed-client>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]>=4.25->torchgeo) (2.8.2)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from kornia>=0.8.2->torchgeo) (0.1.10)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2025.11.12)\n",
            "Requirement already satisfied: hydra-core>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (1.3.2)\n",
            "Requirement already satisfied: lightly_utils~=0.0.0 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (0.0.2)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.32.4)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (1.17.0)\n",
            "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (4.67.1)\n",
            "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.12.3)\n",
            "Requirement already satisfied: pytorch_lightning>=1.0.4 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.6.0)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.5.0)\n",
            "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (3.1.16)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (2025.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (0.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5->torchgeo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5->torchgeo) (2025.2)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo) (25.4.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo) (8.3.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo) (0.7.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo) (1.1.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch>=0.5->torchgeo) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch>=0.5->torchgeo) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch>=0.5->torchgeo) (1.2.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo) (4.9.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.11)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2->torchgeo) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from typeshed-client>=2.8.2->jsonargparse[signatures]>=4.25->torchgeo) (6.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2->torchgeo) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo) (1.22.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.54G/1.54G [01:42<00:00, 15.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed M-33-20-D-c-4-2 1/41\n",
            "Processed M-33-20-D-d-3-3 2/41\n",
            "Processed M-33-32-B-b-4-4 3/41\n",
            "Processed M-33-48-A-c-4-4 4/41\n",
            "Processed M-33-7-A-d-2-3 5/41\n",
            "Processed M-33-7-A-d-3-2 6/41\n",
            "Processed M-34-32-B-a-4-3 7/41\n",
            "Processed M-34-32-B-b-1-3 8/41\n",
            "Processed M-34-5-D-d-4-2 9/41\n",
            "Processed M-34-51-C-b-2-1 10/41\n",
            "Processed M-34-51-C-d-4-1 11/41\n",
            "Processed M-34-55-B-b-4-1 12/41\n",
            "Processed M-34-56-A-b-1-4 13/41\n",
            "Processed M-34-6-A-d-2-2 14/41\n",
            "Processed M-34-65-D-a-4-4 15/41\n",
            "Processed M-34-65-D-c-4-2 16/41\n",
            "Processed M-34-65-D-d-4-1 17/41\n",
            "Processed M-34-68-B-a-1-3 18/41\n",
            "Processed M-34-77-B-c-2-3 19/41\n",
            "Processed N-33-104-A-c-1-1 20/41\n",
            "Processed N-33-119-C-c-3-3 21/41\n",
            "Processed N-33-130-A-d-3-3 22/41\n",
            "Processed N-33-130-A-d-4-4 23/41\n",
            "Processed N-33-139-C-d-2-2 24/41\n",
            "Processed N-33-139-C-d-2-4 25/41\n",
            "Processed N-33-139-D-c-1-3 26/41\n",
            "Processed N-33-60-D-c-4-2 27/41\n",
            "Processed N-33-60-D-d-1-2 28/41\n",
            "Processed N-33-96-D-d-1-1 29/41\n",
            "Processed N-34-106-A-b-3-4 30/41\n",
            "Processed N-34-106-A-c-1-3 31/41\n",
            "Processed N-34-140-A-b-3-2 32/41\n",
            "Processed N-34-140-A-b-4-2 33/41\n",
            "Processed N-34-140-A-d-3-4 34/41\n",
            "Processed N-34-140-A-d-4-2 35/41\n",
            "Processed N-34-61-B-a-1-1 36/41\n",
            "Processed N-34-66-C-c-4-3 37/41\n",
            "Processed N-34-77-A-b-1-4 38/41\n",
            "Processed N-34-94-A-b-2-4 39/41\n",
            "Processed N-34-97-C-b-1-2 40/41\n",
            "Processed N-34-97-D-c-2-4 41/41\n",
            "Dataset size: 7470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/landcoverai_saved\n",
        "!cp -r /content/landcoverai/* /content/drive/MyDrive/landcoverai_saved/\n",
        "\n",
        "print(\"Dataset copied to Drive successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW9KlIFZJJQD",
        "outputId": "e3377fb1-d3d5-4552-ecaa-6ebe3eb50b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cp: cannot stat '/content/landcoverai/*': No such file or directory\n",
            "Dataset copied to Drive successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchgeo.datasets import LandCoverAI\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/landcoverai_saved\"\n",
        "\n",
        "train_ds = LandCoverAI(root=root_dir, split=\"train\", download=False)\n",
        "val_ds   = LandCoverAI(root=root_dir, split=\"val\", download=False)\n",
        "test_ds  = LandCoverAI(root=root_dir, split=\"test\", download=False)\n",
        "\n",
        "print(\"Train:\", len(train_ds))\n",
        "print(\"Val:\", len(val_ds))\n",
        "print(\"Test:\", len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfzjGXVsJURy",
        "outputId": "49222e16-5c82-4834-ebc9-75b1fc74a804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 7470\n",
            "Val: 1602\n",
            "Test: 1602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Dataloaders ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idDzO4GhKcLb",
        "outputId": "e9ba38f6-5d60-4b36-b7bc-8e5ef792b44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "best_model_path = \"/content/drive/MyDrive/PartB_AttUNet_LandCoverAI_best.pth\"\n",
        "\n",
        "model = AttUNet(n_channels=3, n_classes=5, base_ch=32)\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded trained AttUNet model successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dcsU23gKhvw",
        "outputId": "657c59a1-5f02-4164-f10c-e242fdbefdc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded trained AttUNet model successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_iou(pred, true, num_classes=5):\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = (pred == cls)\n",
        "        true_inds = (true == cls)\n",
        "\n",
        "        intersection = (pred_inds & true_inds).sum()\n",
        "        union = (pred_inds | true_inds).sum()\n",
        "\n",
        "        if union == 0:\n",
        "            ious.append(np.nan)\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "\n",
        "    return ious\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader, num_classes=5):\n",
        "    model.eval()\n",
        "    iou_list = []\n",
        "    pixel_acc_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            img = batch[\"image\"].to(device)\n",
        "            mask = batch[\"mask\"].to(device)\n",
        "\n",
        "            logits = model(img)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            preds_np = preds.cpu().numpy().reshape(-1)\n",
        "            mask_np  = mask.cpu().numpy().reshape(-1)\n",
        "\n",
        "            # Pixel accuracy\n",
        "            pixel_acc = (preds_np == mask_np).mean()\n",
        "            pixel_acc_list.append(pixel_acc)\n",
        "\n",
        "            # IoU\n",
        "            ious = compute_iou(preds_np, mask_np, num_classes)\n",
        "            iou_list.append(ious)\n",
        "\n",
        "    mean_pixel_acc = np.mean(pixel_acc_list)\n",
        "    mean_iou = np.nanmean(iou_list, axis=0)\n",
        "    mean_mIoU = np.nanmean(mean_iou)\n",
        "\n",
        "    return mean_pixel_acc, mean_iou, mean_mIoU"
      ],
      "metadata": {
        "id": "LiZbcr8xKk1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_loader(loader, max_batches=20):\n",
        "    \"\"\"Take only a small number of batches for quick evaluation.\"\"\"\n",
        "    for i, batch in enumerate(loader):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "OXSVFHdMe1Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc, val_iou_per_class, val_miou = evaluate_model(\n",
        "    model,\n",
        "    fast_loader(val_loader, max_batches=20)\n",
        ")\n",
        "\n",
        "test_acc, test_iou_per_class, test_miou = evaluate_model(\n",
        "    model,\n",
        "    fast_loader(test_loader, max_batches=20)\n",
        ")\n",
        "\n",
        "print(\"===== FAST VALIDATION METRICS =====\")\n",
        "print(\"Pixel Accuracy:\", val_acc)\n",
        "print(\"mIoU:\", val_miou)\n",
        "print(\"Class-wise IoU:\", val_iou_per_class)\n",
        "\n",
        "print(\"\\n===== FAST TEST METRICS =====\")\n",
        "print(\"Pixel Accuracy:\", test_acc)\n",
        "print(\"mIoU:\", test_miou)\n",
        "print(\"Class-wise IoU:\", test_iou_per_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQdCKM82e43n",
        "outputId": "74c39e6d-e092-4890-fd94-58d12e06fba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== FAST VALIDATION METRICS =====\n",
            "Pixel Accuracy: 0.33119850158691405\n",
            "mIoU: 0.07215405794135644\n",
            "Class-wise IoU: [3.44622710e-01 5.32539406e-04 2.45643959e-05 0.00000000e+00\n",
            " 1.55904755e-02]\n",
            "\n",
            "===== FAST TEST METRICS =====\n",
            "Pixel Accuracy: 0.318635082244873\n",
            "mIoU: 0.06991305548726745\n",
            "Class-wise IoU: [3.36096302e-01 1.50123314e-04 2.35983595e-05 0.00000000e+00\n",
            " 1.32952541e-02]\n"
          ]
        }
      ]
    }
  ]
}
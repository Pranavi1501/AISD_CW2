{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PART B — ADAPTATION TO INDIAN CONTEXT (UTTARAKHAND - HIMALAYAS)**\n",
        "\n",
        "This notebook will cover:\n",
        "\n",
        "1. Context & SDG relevance\n",
        "\n",
        "2. LandCover.ai training dataset (proxy labels)\n",
        "\n",
        "3. Improved Attention U-Net architecture\n",
        "\n",
        "4. Training on proxy labels\n",
        "\n",
        "5. Applying model to Sentinel-2 imagery (Joshimath)\n",
        "\n",
        "6. NDVI-guided inference\n",
        "\n",
        "7. Yearly change detection maps (2016-2023)\n"
      ],
      "metadata": {
        "id": "UFBermL7dItY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE ON TRAINING RUNTIME / GPU LIMITS\n",
        "\n",
        "The experimental design for this TensorFlow Attention U-Net was to train\n",
        "for 25 epochs on all available LandCover.ai tiles (batch_size=4).\n",
        "\n",
        "On Google Colab, the complete 25-epoch run could not always be finished\n",
        "due to GPU runtime limits and session resets. However, the *identical*\n",
        "architecture, tiling strategy, and loss function were successfully trained\n",
        "end-to-end in the PyTorch implementation (myaisd_cw2_part2.py), where a\n",
        "12-epoch run converged and achieved stable validation mIoU.\n",
        "\n",
        "Therefore, AISD_CW2_partB.py documents the full TensorFlow pipeline and the\n",
        "intended experiment, while myaisd_cw2_part2.py provides the fully completed\n",
        "training run used for quantitative analysis in the report."
      ],
      "metadata": {
        "id": "SRqzDkfNlR-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_ROOT = \"/content/LandCoverAI\"\n",
        "RAW_DIR = \"/content/LandCoverAI/raw\"\n",
        "TILES_IMG_DIR = \"/content/LandCoverAI/tiles/images\"\n",
        "TILES_MASK_DIR = \"/content/LandCoverAI/tiles/masks\"\n",
        "\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(TILES_IMG_DIR, exist_ok=True)\n",
        "os.makedirs(TILES_MASK_DIR, exist_ok=True)\n",
        "\n",
        "DATA_ROOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lGrH8D83drqZ",
        "outputId": "fc1f9a6a-86d5-4d52-a3e5-c25c550ee6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/LandCoverAI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Context & SDG Motivation (India - Uttarakhand Himalayas)\n",
        "\n",
        "The Himalayan state of Uttarakhand has undergone extensive land-use change and forest loss, particularly around Joshimath, where deforestation-driven slope instability contributed directly to the 2023 land subsidence crisis.\n",
        "This region is relevant to multiple SDGs:\n",
        "\n",
        "SDG 13 - Climate Action\n",
        "\n",
        "SDG 15 - Life on Land\n",
        "\n",
        "SDG 11 - Sustainable Cities & Communities\n",
        "\n",
        "The goal of Part B is to adapt the Attention U-Net model from the Amazon rainforest dataset to the Indian Himalayan context, where official high-resolution annotated datasets are unavailable.\n",
        "Therefore, I use LandCover.ai as a proxy training dataset and apply the trained model to Sentinel-2 imagery of Joshimath to detect vegetation loss over time."
      ],
      "metadata": {
        "id": "JhBtcrZceGDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/LandCoverAI/landcoverai.zip \\\n",
        "https://landcover.ai.linuxpolska.com/download/landcover.ai.v1.zip"
      ],
      "metadata": {
        "id": "s3yQYeJNeppP",
        "outputId": "8dd11b02-56aa-4ace-d67a-3d59c648bf21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-10 00:25:37--  https://landcover.ai.linuxpolska.com/download/landcover.ai.v1.zip\n",
            "Resolving landcover.ai.linuxpolska.com (landcover.ai.linuxpolska.com)... 195.78.67.65\n",
            "Connecting to landcover.ai.linuxpolska.com (landcover.ai.linuxpolska.com)|195.78.67.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1538212277 (1.4G) [application/zip]\n",
            "Saving to: ‘/content/LandCoverAI/landcoverai.zip’\n",
            "\n",
            "/content/LandCoverA 100%[===================>]   1.43G  10.3MB/s    in 2m 22s  \n",
            "\n",
            "2025-12-10 00:28:00 (10.3 MB/s) - ‘/content/LandCoverAI/landcoverai.zip’ saved [1538212277/1538212277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/LandCoverAI/landcoverai.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(RAW_DIR)\n",
        "\n",
        "print(\"Extracted into:\", RAW_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFppVC76g8sB",
        "outputId": "5cd153f4-f58f-43df-af86-f8e04b5b9e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted into: /content/LandCoverAI/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LandCover.ai originally contains 41 BIG orthophotos -\n",
        "\n",
        "Each orthophoto is huge: Up to 9000 × 9500 px, RGB GeoTIFF and paired segmentation mask"
      ],
      "metadata": {
        "id": "YIeTVqpPhlWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "\n",
        "RAW_IMG_DIR = \"/content/LandCoverAI/raw/images/\"\n",
        "RAW_MASK_DIR = \"/content/LandCoverAI/raw/masks/\"\n",
        "\n",
        "img_paths = sorted(glob.glob(RAW_IMG_DIR + \"*.tif\"))\n",
        "mask_paths = sorted(glob.glob(RAW_MASK_DIR + \"*.tif\"))\n",
        "\n",
        "TILE_SIZE = 512\n",
        "\n",
        "for img_path, mask_path in zip(img_paths, mask_paths):\n",
        "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    k = 0\n",
        "\n",
        "    for y in range(0, h, TILE_SIZE):\n",
        "        for x in range(0, w, TILE_SIZE):\n",
        "            tile_img = img[y:y+TILE_SIZE, x:x+TILE_SIZE]\n",
        "            tile_mask = mask[y:y+TILE_SIZE, x:x+TILE_SIZE]\n",
        "\n",
        "            if tile_img.shape[0] == TILE_SIZE and tile_img.shape[1] == TILE_SIZE:\n",
        "                cv2.imwrite(f\"{TILES_IMG_DIR}/{img_name}_{k}.jpg\", tile_img)\n",
        "                cv2.imwrite(f\"{TILES_MASK_DIR}/{img_name}_{k}_m.png\", tile_mask)\n",
        "            k += 1\n",
        "\n",
        "    print(f\"Processed {img_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g9z2njLlhzAz",
        "outputId": "9c28860c-bbd3-47f9-9e15-736face3df32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed M-33-20-D-c-4-2\n",
            "Processed M-33-20-D-d-3-3\n",
            "Processed M-33-32-B-b-4-4\n",
            "Processed M-33-48-A-c-4-4\n",
            "Processed M-33-7-A-d-2-3\n",
            "Processed M-33-7-A-d-3-2\n",
            "Processed M-34-32-B-a-4-3\n",
            "Processed M-34-32-B-b-1-3\n",
            "Processed M-34-5-D-d-4-2\n",
            "Processed M-34-51-C-b-2-1\n",
            "Processed M-34-51-C-d-4-1\n",
            "Processed M-34-55-B-b-4-1\n",
            "Processed M-34-56-A-b-1-4\n",
            "Processed M-34-6-A-d-2-2\n",
            "Processed M-34-65-D-a-4-4\n",
            "Processed M-34-65-D-c-4-2\n",
            "Processed M-34-65-D-d-4-1\n",
            "Processed M-34-68-B-a-1-3\n",
            "Processed M-34-77-B-c-2-3\n",
            "Processed N-33-104-A-c-1-1\n",
            "Processed N-33-119-C-c-3-3\n",
            "Processed N-33-130-A-d-3-3\n",
            "Processed N-33-130-A-d-4-4\n",
            "Processed N-33-139-C-d-2-2\n",
            "Processed N-33-139-C-d-2-4\n",
            "Processed N-33-139-D-c-1-3\n",
            "Processed N-33-60-D-c-4-2\n",
            "Processed N-33-60-D-d-1-2\n",
            "Processed N-33-96-D-d-1-1\n",
            "Processed N-34-106-A-b-3-4\n",
            "Processed N-34-106-A-c-1-3\n",
            "Processed N-34-140-A-b-3-2\n",
            "Processed N-34-140-A-b-4-2\n",
            "Processed N-34-140-A-d-3-4\n",
            "Processed N-34-140-A-d-4-2\n",
            "Processed N-34-61-B-a-1-1\n",
            "Processed N-34-66-C-c-4-3\n",
            "Processed N-34-77-A-b-1-4\n",
            "Processed N-34-94-A-b-2-4\n",
            "Processed N-34-97-C-b-1-2\n",
            "Processed N-34-97-D-c-2-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tiled Images:\", len(os.listdir(TILES_IMG_DIR)))\n",
        "print(\"Tiled Masks:\", len(os.listdir(TILES_MASK_DIR)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg-PRICah83_",
        "outputId": "c03dcb60-b593-4069-c1a2-7a8d1b461081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiled Images: 10674\n",
            "Tiled Masks: 10674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# session ram crashed so changing to a Lightweight Data Generator, will load only what is needed per batch.\n",
        "import glob\n",
        "\n",
        "TILES_IMG_DIR = \"/content/LandCoverAI/tiles/images/\"\n",
        "TILES_MASK_DIR = \"/content/LandCoverAI/tiles/masks/\"\n",
        "\n",
        "image_paths = sorted(glob.glob(TILES_IMG_DIR + \"*.jpg\"))\n",
        "mask_paths = sorted(glob.glob(TILES_MASK_DIR + \"*_m.png\"))\n",
        "\n",
        "print(\"Images:\", len(image_paths))\n",
        "print(\"Masks:\", len(mask_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q59Q7bYdty_S",
        "outputId": "3b3bceb2-64e8-42e2-e389-dda25d474ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: 10674\n",
            "Masks: 10674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_img, val_img, train_mask, val_mask = train_test_split(\n",
        "    image_paths, mask_paths, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "1qZNfsXwt3Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#augmentation and data generator that loads files on demand\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "def tile_generator(img_paths, mask_paths, batch_size=4):\n",
        "    while True:\n",
        "        idx = np.random.choice(len(img_paths), batch_size)\n",
        "\n",
        "        batch_imgs = []\n",
        "        batch_masks = []\n",
        "\n",
        "        for i in idx:\n",
        "            img = cv2.imread(img_paths[i], cv2.IMREAD_COLOR)\n",
        "            mask = cv2.imread(mask_paths[i], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            mask = (mask > 0).astype(\"float32\")\n",
        "\n",
        "            batch_imgs.append(img)\n",
        "            batch_masks.append(mask)\n",
        "\n",
        "        batch_imgs = np.array(batch_imgs)\n",
        "        batch_masks = np.array(batch_masks).reshape(-1, 512, 512, 1)\n",
        "\n",
        "        yield batch_imgs, batch_masks"
      ],
      "metadata": {
        "id": "dGfFDnPIt42G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_gen = tile_generator(train_img, train_mask, batch_size=batch_size)\n",
        "val_gen   = tile_generator(val_img, val_mask, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "2VHcIhapvlPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Attention Gate\n",
        "def attention_block(x, g, inter_channels):\n",
        "    theta_x = layers.Conv2D(inter_channels, (2, 2), strides=(2, 2), padding='same')(x)\n",
        "    phi_g = layers.Conv2D(inter_channels, (1, 1), padding='same')(g)\n",
        "\n",
        "    add_xg = layers.Activation('relu')(layers.add([theta_x, phi_g]))\n",
        "    psi = layers.Conv2D(1, (1, 1), padding='same')(add_xg)\n",
        "    psi = layers.Activation('sigmoid')(psi)\n",
        "    psi_up = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(psi)\n",
        "\n",
        "    return layers.multiply([x, psi_up])\n",
        "\n",
        "# Attention U-Net\n",
        "def attention_unet(input_shape=(512, 512, 3)):\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    # Bottleneck\n",
        "    bn = layers.Conv2D(1024, 3, activation='relu', padding='same')(p4)\n",
        "    bn = layers.Conv2D(1024, 3, activation='relu', padding='same')(bn)\n",
        "\n",
        "    # Decoder + Attention\n",
        "    g1 = layers.Conv2D(512, 1)(bn)\n",
        "    att1 = attention_block(c4, g1, 256)\n",
        "    u1 = layers.UpSampling2D((2, 2))(bn)\n",
        "    u1 = layers.concatenate([u1, att1])\n",
        "    c5 = layers.Conv2D(512, 3, activation='relu', padding='same')(u1)\n",
        "    c5 = layers.Conv2D(512, 3, activation='relu', padding='same')(c5)\n",
        "\n",
        "    g2 = layers.Conv2D(256, 1)(c5)\n",
        "    att2 = attention_block(c3, g2, 128)\n",
        "    u2 = layers.UpSampling2D((2, 2))(c5)\n",
        "    u2 = layers.concatenate([u2, att2])\n",
        "    c6 = layers.Conv2D(256, 3, activation='relu', padding='same')(u2)\n",
        "    c6 = layers.Conv2D(256, 3, activation='relu', padding='same')(c6)\n",
        "\n",
        "    g3 = layers.Conv2D(128, 1)(c6)\n",
        "    att3 = attention_block(c2, g3, 64)\n",
        "    u3 = layers.UpSampling2D((2, 2))(c6)\n",
        "    u3 = layers.concatenate([u3, att3])\n",
        "    c7 = layers.Conv2D(128, 3, activation='relu', padding='same')(u3)\n",
        "    c7 = layers.Conv2D(128, 3, activation='relu', padding='same')(c7)\n",
        "\n",
        "    g4 = layers.Conv2D(64, 1)(c7)\n",
        "    att4 = attention_block(c1, g4, 32)\n",
        "    u4 = layers.UpSampling2D((2, 2))(c7)\n",
        "    u4 = layers.concatenate([u4, att4])\n",
        "    c8 = layers.Conv2D(64, 3, activation='relu', padding='same')(u4)\n",
        "    c8 = layers.Conv2D(64, 3, activation='relu', padding='same')(c8)\n",
        "\n",
        "    outputs = layers.Conv2D(1, (1, 1), padding='same', activation='sigmoid')(c8)\n",
        "\n",
        "    return Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "_NgQJlDQwyoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = attention_unet()"
      ],
      "metadata": {
        "id": "ByN6moM0wDpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tversky Index\n",
        "def tversky(y_true, y_pred, alpha=0.7):\n",
        "    y_true_pos = tf.reshape(y_true, [-1])\n",
        "    y_pred_pos = tf.reshape(y_pred, [-1])\n",
        "    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = tf.reduce_sum(y_true_pos * (1 - y_pred_pos))\n",
        "    false_pos = tf.reduce_sum((1 - y_true_pos) * y_pred_pos)\n",
        "    return (true_pos + 1e-6) / (\n",
        "        true_pos + alpha * false_neg + (1 - alpha) * false_pos + 1e-6\n",
        "    )\n",
        "\n",
        "# 2. Focal Tversky Loss\n",
        "def focal_tversky_loss(y_true, y_pred):\n",
        "    return tf.pow((1 - tversky(y_true, y_pred)), 1.3)\n",
        "\n",
        "# 3. Combo Loss (Focal Tversky + BCE)\n",
        "def combo_loss(y_true, y_pred):\n",
        "    ft = focal_tversky_loss(y_true, y_pred)\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    return 0.7 * ft + 0.3 * bce"
      ],
      "metadata": {
        "id": "fF8U54eoxZ3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=combo_loss,\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "KhNUt_ohvnTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.3,\n",
        "    patience=4,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "fLRrwq5fyPjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = len(train_img) // batch_size\n",
        "val_steps = len(val_img) // batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=val_steps,\n",
        "    epochs=25,\n",
        "    callbacks=[lr_callback],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxD_yyTNx0fW",
        "outputId": "87222aa8-cf77-449e-b224-961ec3d43224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m 603/2134\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24:55\u001b[0m 977ms/step - accuracy: 0.4249 - loss: 0.5396"
          ]
        }
      ]
    }
  ]
}